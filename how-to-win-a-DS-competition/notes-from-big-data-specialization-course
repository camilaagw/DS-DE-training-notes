# Recap of main ML algorithms

**Linear models**
 Split space into 2 subspaces with a hyperplane (Ex.: Logistic Reg., SVM). Good for sparse, high dimensional data, where there are linear dependencies.

**Tree-based methods**
Decision tree is the main construction block. Uses divide and conquer approach to recursively divide the space into subspaces (or boxes). In regression tasks aproximates points in the boxes with a constant. A bad choise when data has linear dependencies.

**kNN-based methods**
Points close to each other are likely to have the same labels. Heavily rely on how to measure points "closeness"

**Neural Networks** 
Produce smooth separating curve (in contrast to decision trees).

No free lunch theorem: There is no method which outperforms all others for all tasks. There is no "silver bullet algorithm"

# Feature processing

Preprocessing and generation pipelines depend on a model type.

For instance:
- Lineal models require hot-encoded variables if the dependencies between numerical features and target are not linear. 
- Decision-trees-based models do not need one hot encoding. These models also don't depend on feature scaling
- 

## Numeric features

### Preprocessing

#### Feature scaling:
We use preprocessing to scale all features to one scale, so that their initial impact on the model will be roughly similar.

Decision-trees-based models don't depend on feature scaling while Linear Models , KNN and neural networks do (regularization impact is proportional to feature scale and  gradient descent methods will go crazy without the proper scaling)

Types: 
- Min max scaling (distributions do not change)
- Standar scaler (Goal: mean==0 and std==1)


#### Outliers treatment
Outliers affect linear models. To prevent them we can use winzorisation: clip feature values ammong a low and a high percentile

#### Rank transformation
Set spaces between sorted feature values to be equal. Can be also used to treat outliers as with the rank they will be closer to other values. Linear models, KNN and NN can also benefit from this transformation. Can be applied to train+test data

#### Other transformations
These transformations drive too big values to the features average value. Besides, values closer to zero become more distinguishable 
- Log transform: `np.log(1+x)`
- Raising to the power <1: `np.sqrt(x+2/3)`

TIP: use ensemble methods with different preprocessings to achieve better results

### Features generation
Examples:
- Interactions: cLick rate, price per m2, calculated distances with Pytagoras theorem. In adition to Linear Models, these are beneficial for methods such as GBT as they experience dificultiexs with approximations of multiplications and divissions
- Fractional part (i.e. of prices)


## Categorical and ordinal features

**Label encoding**
Map each category to an unique value. Useful for tree-based models. Not always useful for linear models or NN unless the values assigned are correlated with the target. Types:
- Alphabetical order
- Order or apearance
- Meaningful order (in case the categorical variable is ordinal)
- Frequency encoding: Map each category to its frequecy. For multiple categories with the same frequency we can use a rank operation 

**One-hot encoding**
Creates a new column for every category. Works better with non-tree-based models than with tree-based ones due to the explosion in the number of features


## Datetime and coordinates

### Datetime 

1. Periodicity: Day number in week, month, season, year, second, minute, hour.
2. Time since
	a. Row-independent moment
	For example: since 00:00:00 UTC, 1 January 1970;
	b. Row-dependent important moment
	Number of days left until next holidays/ time passed
	after last holiday.
3. Difference between dates: datetime_feature_1 - datetime_feature_2

### Coordinates

Ideas:
- Calculate distance to the nearest shop, hospital, school, etc
- Organize data into clusters and add distance to the center of clusters
- Compute aggregated stats per area: Price per m2, number of flats
with tree based models, you can add rotations of the coordinates as new features

## Handling missing values

Different methods to fill the missing vaues: 
- Replace them by other value (999, -1) (decision-based models)
- Mean, median  (linear models, NN,  KNN)
- Recontruct values (time series)
- Add Isnull feature

Note 1 : Avoid filling missing values before feature generation 
Note 2: Finding hidden missing values encoded as a numerial value: Plot histogram and look for abnormal peaks!
Note 3: XgBoost can handle NaN

# Exploratory data analysis

General advice: 
- Get domaing knowledge
- Check if the data is intuitive and consistent with its definition
- It is crucial to understand how the data was generated to set up a proper validation scheme. For instance if the distribution of the test set is different from the training set, we cannot validate our model with a part of the training set
- Visualization is important: Patterns lead to questions (or hypothesis) and hypothesis can be supported by graphics

Useful Pandas methods: `df.types`, `df.info()`, `df.describe()`, `x._value_counts()`, `x.isnull()`

Useful visualization tools:
- Histograms (be careful with bins!):  `plt.hist(x)`
- Plot index vs value: `plt.plot(x,'.')` 
- Plot index vs value colored by target value: `plt.scatter(range(len(x)),x ,c=y)`
- Exploring features relations: `plt.scatter(x1,x2)`, `pd.scatter_matrix(df)`
- Correlation matrices: `plt.matshow()` (TODO: add clustering example)
- Plot mean values of individual features: `df.mean().sort_values().plot(style='.')

## Dataset cleaning and other things to check

- Remove features with zero variance in training set
- Check for dupllicated rows (check if same rows have same label)
- Check for duplicated columns:
```
for col in categorical_cols:
	train_test[col] = train_test[col].factorize()[0]

train_test.T.drop_duplicates
```

# Validation strategies

There are three main validation strategies:
1. Holdout (i.e. 70% for training, 30% for testing). Use if validation results and/or optimal parameters among different splits do not change "too much"
2. KFold. Use if scores and optimal parameters differ for different splits.
3. Leave One Out (iterate though every point in your data). Use if you have too little data.
Stratification preserve the same target distribution over
different folds

Data slitting stategies:
- Random-based splitting
- Time-based splitting. Special case: Window moving validation.  
- ID-based splitting

Note: Ensure similar distributions between your validation set and the test set (In most cases, the reality!)

### Specific advice for Kaggle competitions: 

 If we have big dispersion of scores on validation stage, we should do extensive validation
– Average scores from different KFold splits
– Tune model on one split, evaluate score on the other

If submission’s score do not match local validation score,we should
– Check if we have too little data in public LB
– Check if we overfitted
– Check if we chose correct splitting strategy
– Check if train/test have different distibutions

Expect LB shuffle because of
– Randomness
– Little amount of data
– Different public/private distributions


# Metrics optimization

# Advanced feature engineering


