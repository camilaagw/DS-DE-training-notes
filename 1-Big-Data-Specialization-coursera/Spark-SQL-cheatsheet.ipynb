{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "spark =(SparkSession\n",
    "        .builder\n",
    "        .enableHiveSupport()\n",
    "        .appName(\"Spark sql\")\n",
    "        .master(\"local\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From RDD + SchemaÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd = spark.sparkContext.textFile(\"my_text_file.csv\").map(lambda x: x.split(\", \"))\n",
    "\n",
    "shema = (StructType()\n",
    "         .add(\"ip\", StringType())\n",
    "         .add(\"code\", StringType())\n",
    "         .add(\"country\", StringType())\n",
    "         )\n",
    "\n",
    "df = spark.createDataFrame(my_rdd, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select country,ip, code from web.access_logs\")\n",
    "spark.read.table(\"web.access_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.json(\"json_file\")\n",
    "spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"csv_file\") # Schema should be provided here\n",
    "spark.read.parquet(\"parquet_file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection and filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .select(\"country\", \"id_\", \"counts\", \"http_code\", \"user_agent\")\n",
    " .withColumnRenamed(\"id_\", \"id\")\n",
    " .where(\"country = Colombia\")\n",
    " .where(\n",
    "     (df.http_code <> '200') &\n",
    "     (df.user_agent.like('%Android%')))\n",
    " .where(f.col(\"id\") < f.col(\"counts\") )\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a Pandas-like interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['country','ip']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.http_code <> '200']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"user_agent\", f.length(\"user_agent\").alias(\"len\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"url\", f.concat(f.lit(\"http://vk.com\"), \"url\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .select(\"user_agent\", f.split(\"user_agent\", \" \").alias(\"list\"))\n",
    " .select(\"user_agent\", f.explode(\"list\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .select(\"user_agent\",\n",
    "        f.when(df.user_agent.like(\"%Android%\"), \"Android\")\n",
    "         .when(df.user_agent.like(\"%Windows%\"), \"Windows\")\n",
    "         .otherwise(\"Other\")\n",
    "         .alias(\"OS\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .groupBy(\"url\")\n",
    " .agg(f.count(\"ip\")) # or .agg({\"ip\":\"count\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .groupBy(\"url\")\n",
    " .agg(f.count(\"*\").alias(\"count\"),\n",
    "      f.countDistict(\"ip\"),\n",
    "      f.min(\"response_length\"),\n",
    "      f.max(\"response_length\"),\n",
    "      f.avg(\"response_length\").astype(\"int\")) \n",
    " .orderBy(f.col(\"count\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.join(df2, on = df1.ip == df2.ip, how='left')\n",
    "# how options: inner (default), left, right, left_semi, left_anti etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.crossJoin(df).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function():\n",
    "    \"\"\"Decompose url\"\"\"\n",
    "    pass\n",
    "\n",
    "my_function_udf = f.udf(my_function, ArrayType(StringType()))\n",
    "\n",
    "df.select(my_function_udf(\"url\").alias(\"url_decomposed\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UDF from a python object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import user_agents as ua\n",
    "\n",
    "get_browser_udf = f.udf(lambda x: ua.parse(x).browser.family)\n",
    "get_os_udf      = f.udf(lambda x: ua.parse(x).os.family)\n",
    "get_device_ud   = f.udf(lambda x: ua.parse(x).device.family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register a UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.register(\"my_function\",\n",
    "               my_function,\n",
    "               ArrayType(StringType()))\n",
    "\n",
    "spark.sql(\"select my_function(url) as url_decomposed fron web.access_log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "String to Unixtime: Time in seconds since 1970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"unixtime\", f.unix_timestamp(\"time\",\n",
    "                                          \"dd/MM/yyyy:HH:mm:hh Z\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unix to timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"timestamp\", f.col(\"unixtime\").astype(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .groupby(\"ip\")\n",
    " .agg(f.min(\"timestamp\").alias(\"begin\"),\n",
    "      f.max(\"timestamp\").alias(\"end\"))\n",
    " .select(\"ip\", (f.datediff(\"end\", \"begin\")).alias(\"days_cnt\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute aggregations over a speficic row while the number of rows of the original df remains unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_window = Window.partitionBy(\"ip\").orderBy(\"unixtime\")\n",
    "\n",
    "df.select(\n",
    "    \"ip\",\n",
    "    \"time\",\n",
    "    f.count(\"*\").over(my_window).alias(\"cnt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples of functions used with windows: `first`, `last`, `lag`, `lead`, `row_number`, `min`, `max`, `sum`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.select(\"ip\", \"unixtime\",\n",
    "          f.row_number().over(my_window).alias(\"count\"),\n",
    "          f.lag(\"unixtime\").over(my_window).alias(\"lag\"),\n",
    "          f.lead(\"unixtime\").over(my_window).alias(\"lead\")\n",
    "          )\n",
    "   .select(\"ip\", \"unixtime\",\n",
    "          (f.col(\"lead\") - f.col(\"unixtime\")).alias(\"diff\"))\n",
    "   .where(\"diff >= 18000 or diff is NULL\")\n",
    "   .groupBy(\"ip\").count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different way to use windows with time attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .selectExpr(\"country\", \"(users * counts) as total_count\" , \"ip\")\n",
    " .groupBy(col(\"country\"), window(col(\"SomeDate\"), \"1 day\"))\n",
    " .sum(\"total_count\")\n",
    " .show(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot table "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pivot(column, [values])` is called between `groupBy` and `count`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"ip\").pivot(\"url\", top_url_list).count().fillna(0)\n",
    "# Num. resulting of columns: len(top_url_list) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Create Hive views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createTempView(\"geoip\")\n",
    "# See view created\n",
    "spark.catalog.listTables(\"web\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"create table web.geoip as select from geoip\")\n",
    "# See table created\n",
    "spark.catalog.listTables(\"web\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hive table\n",
    "df.write.saveAsTable(\"table_name\",\n",
    "                     mode='overwrite', # other options: error, append\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data in binary mode\n",
    "df.write.save(\"table_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.save(\"table_name\", format='csv', mode='overwrite') # format='json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet(\"table_name\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(\"table_name\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to an external DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"jdbc:mysql://localhost:3306/demo?user=demo&password=demo\"\n",
    "# Write df\n",
    "df.write.jdbc(connection_string, \"geoip\")\n",
    "# Read df\n",
    "new_df = spark.read.jdbc(connection_string, \"geoip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
